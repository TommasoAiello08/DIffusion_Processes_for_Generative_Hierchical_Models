# Diffusion Processes for Generative Hierarchical Models

This repository accompanies a thesis investigating **phase transitions in denoising diffusion models** applied to hierarchical data generated by formal grammars.

## ğŸ¯ Overview

This work investigates the claim from *"A Phase Transition in Diffusion Models Reveals the Hierarchical Nature of Data"* by Sclocchi, Favero, and Wyart, empirically examining whether phase transitions in reconstruction occur generally across hierarchical structures.

The repository provides a controlled setting using **Random Hierarchical Models (RHMs)**, where synthetic data is generated by context-free grammars of varying depth and branching factors. By studying how diffusion models denoise these sequences, we probe the interplay between **hierarchy, noise, and reconstruction accuracy**.

## ğŸ“ Repository Structure

```
.
â”œâ”€â”€ src/                          # Core source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ grammar.py               # HierarchicalGrammar and data generation
â”‚   â”œâ”€â”€ diffusion.py             # Forward diffusion process
â”‚   â”œâ”€â”€ models.py                # Transformer-based denoisers
â”‚   â”œâ”€â”€ utils.py                 # Grammar constraints and utilities
â”‚
â”œâ”€â”€ experiments/                  # Experiment runners
â”‚   â””â”€â”€ run_experiment.py        # Main experiment script
â”‚
â”œâ”€â”€ configs/                      # Configuration files
â”‚   â”œâ”€â”€ grammars.py              # Grammar specifications
â”‚   â””â”€â”€ experiment_config.py     # Experiment configurations
â”‚
â”œâ”€â”€ notebooks/                    # Clean example notebooks
â”‚   â”œâ”€â”€ 01_data_generation.ipynb
â”‚   â”œâ”€â”€ 02_diffusion_basics.ipynb
â”‚   â””â”€â”€ 03_model_training.ipynb
â”‚
â”œâ”€â”€ data/                         # Data directory (gitignored)
â”œâ”€â”€ outputs/                      # Experiment outputs (gitignored)
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

## ğŸš€ Quick Start

### Installation

1. Clone the repository:
```bash
git clone https://github.com/TommasoAiello08/DIffusion_Processes_for_Generative_Hierchical_Models.git
cd DIffusion_Processes_for_Generative_Hierchical_Models
```

2. Create a virtual environment and install dependencies:
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```

### Running Experiments

Run a basic experiment:
```bash
python experiments/run_experiment.py
```

The script will:
1. Generate synthetic hierarchical data according to a grammar
2. Apply forward diffusion (add noise)
3. Train a Transformer-based denoiser
4. Save results to `outputs/`

### Exploring Notebooks

Launch Jupyter and explore the example notebooks:
```bash
jupyter notebook notebooks/
```

The notebooks provide step-by-step walkthroughs of:
- Data generation with hierarchical grammars
- Forward diffusion visualization
- Model training and evaluation

## ğŸ”¬ Key Components

### Hierarchical Grammars (`src/grammar.py`)

Generate structured sequences using context-free grammars:
```python
from src.grammar import HierarchicalGrammar
from configs.grammars import BINARY_3_LEVEL_GRAMMAR

grammar = HierarchicalGrammar(BINARY_3_LEVEL_GRAMMAR)
df = grammar.generate_dataset(n_samples=1000)
```

### Diffusion Process (`src/diffusion.py`)

Forward diffusion with multiple noise schedules:
```python
from src.diffusion import DiffusionProcess

diffusion = DiffusionProcess(noise_scale=0.3, max_time=10.0, mode="cos")
noisy_data = diffusion.add_noise(clean_data, timestep=5.0)
```

### Transformer Models (`src/models.py`)

Encoder-decoder architectures for denoising:
```python
from src.models import TransformerDenoiser_for_denoise

model = TransformerDenoiser_for_denoise(
    d=vocab_size, n=seq_length,
    embed_dim=128, num_heads=8, ff_dim=512
)
```

## ğŸ“Š Experiment Configurations

Predefined configurations are available in `configs/experiment_config.py`:

- **SMALL_EXPERIMENT**: Quick test with 1000 samples
- **MEMORIZATION_EXPERIMENT**: Large dataset (10k samples) to study memorization
- **GENERALIZATION_EXPERIMENT**: Balanced setup for generalization analysis

Modify these or create custom configurations for your experiments.

## ğŸ§ª Research Questions

This repository enables investigation of:

1. **Phase Transitions**: Do reconstruction accuracies show sharp transitions at specific noise levels?
2. **Memorization vs. Generalization**: How does dataset size affect model behavior?
3. **Hierarchy Depth**: Does the number of grammar levels affect denoising difficulty?
4. **Grammar Constraints**: Can models learn to respect hierarchical rules?

## ğŸ“ Citation

If you use this code in your research, please cite:

```bibtex
@mastersthesis{aiello2025diffusion,
  author  = {Tommaso Aiello},
  title   = {Diffusion Processes for Generative Hierarchical Models},
  school  = {[Your University]},
  year    = {2025}
}
```

## ğŸ¤ Contributing

This is a thesis project, but suggestions and improvements are welcome! Feel free to:
- Open issues for bugs or questions
- Submit pull requests for enhancements
- Share your experimental results

## ğŸ“„ License

This project is available under the MIT License - see LICENSE file for details.

## ğŸ™ Acknowledgments

- Inspired by the work of Sclocchi, Favero, and Wyart on phase transitions in diffusion models
- Built with PyTorch, NumPy, and the scientific Python ecosystem

---

**Author**: Tommaso Aiello  
**Contact**: [Your Email/GitHub]  
**Date**: December 2025
